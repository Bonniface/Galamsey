---
title: "ENSC-520 rgee Case Study"
author: "Anthony Holmes"
date: "`r Sys.Date()`"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
```{r, results=FALSE, warning=FALSE,message=FALSE}
library(metan)
library(scales)
library(fable)
library(tsibble)
library(feasts)
library(fable.prophet)
library(ggforce)
library(imputeTS)
library(purrr)
library(leaflet)
library(sf)
library(tibble)
library(ggplot2)
library(googledrive)
library(forecast)
library(lubridate)
library(magrittr)
library(tidyverse)
library(geojsonio)
library(vars)
library(rgee)
library(lmtest)
library(skimr)
library(visdat)
library(car)
library(metan)
library(TSstudio)
library(urca)
library(vars)
library(reticulate)
library(kableExtra)
```

```{r}
ee_check()

ee_Initialize("kalong",drive = TRUE) # initialize GEE,
#this will have you log in to Google Drive
```
```{r}
cc <- read_sf('Ghana shp file/ROI/new_roi.shp')
aoi <- st_transform(cc, st_crs(4326))
aoi.ee <- st_bbox(aoi) %>%
  st_as_sfc() %>%
  sf_as_ee() #Converts it to an Earth Engine Object
```
`


```{r}
getQABits <- function(image, qa) {
  # Convert binary (character) to decimal (little endian)
  qa <- sum(2^(which(rev(unlist(strsplit(as.character(qa), "")) == 1))-1))
  # Return a mask band image, giving the qa value.
  image$bitwiseAnd(qa)$lt(1)
}

mod.clean <- function(img) {
  # Extract the NDVI band
  ndvi_values <- img$select("NDVI")
  # Extract the quality band
  ndvi_qa <- img$select("SummaryQA")
  # Select pixels to mask
  quality_mask <- getQABits(ndvi_qa, "11")
  # Mask pixels with value zero.
  ndvi_values$updateMask(quality_mask)$divide(ee$Image$constant(10000)) #0.0001 is the MODIS Scale Factor
}
Date <- Sys.Date()

modis.evi <- ee$ImageCollection("MODIS/006/MOD13Q1")$filter(ee$Filter$date('2000-01-01',rdate_to_eedate(Date)))$map(mod.clean)
```


```{r}
cc.proj <- st_transform(cc, st_crs(2992))
hex <- st_make_grid(x = cc.proj, cellsize = 17080, square = FALSE) %>%
st_sf() %>%
rowid_to_column('hex_id')
hex <- hex[cc.proj,]
plot(hex)
```
```{r, eval = FALSE}
{
# cc.evi <- ee_extract(x = modis.evi, y = hex["hex_id"], sf = FALSE, scale = 250, fun = ee$Reducer$mean(), via = "drive", quiet = T)
# evi.df <- as.data.frame(cc.evi)
# write.csv(x = evi.df, file = "Data/rgeedf.csv")
}

cc.evi = evi.df <-read.csv("Data/rgeedf.csv")
colnames(evi.df) <- c('hex_id', stringr::str_replace_all(substr(colnames(evi.df[, 2:ncol(evi.df)]), 2, 11), "_", "-"))
```



```{r}
{
  evi.hw.lst <- list() #Create an empty list, this will be used to house the time series projections for each cell. 
evi.dcmp.lst <- list() #Create an empty list, this will be used to house the time series decomposition for each cell.
evi.df<-evi.df[,-2]
evi.trend <- data.frame(hex_id = evi.df$hex_id, na.cnt = NA, na.cnt.2 = NA, trend = NA, p.val = NA, r2 = NA, std.er = NA, trnd.strngth = NA, seas.strngth = NA) #This data frame will hold the trend data
Dates <- data.frame(date = seq(as.Date('2000-01-01'), Date, "month"))
Dates$month <- month(Dates$date)
Dates$year <- year(Dates$date)
i <- 1
}
tsv <- data.frame(evi = t(evi.df[i, 2:ncol(evi.df)])) #converting the data to a transposed data frame
colnames(tsv) <- c("evi")
# 
# write.csv(tsv,"Data/tsv.csv")
# tsv <- read.csv("Data/tsv.csv")
# tsv<-tsv[!(tsv$X=="ex-id"),]
# tsv$X <- as.Date(tsv$X,tryFormats = c("%Y-%m-%d", "%Y/%m/%d"))

head(tsv) #let's take a look
```


```{r}

na.cnt <- length(tsv[is.na(tsv)]) #We want to get an idea of the number of entries with no EVI value
evi.trend$na.cnt[i] <- na.cnt
td <- tsv %>% 
  mutate(month = month(as.Date(rownames(tsv))), year = year(as.Date(rownames(tsv)))) %>% 
  group_by(year, month) %>%
  summarise(mean_evi = mean(evi, na.rm = T), .groups = "keep") %>%
  as.data.frame()
write.csv(td,"Data/td.csv")
head(td)
```



```{r}
td$date <- as.Date(paste0(td$year, "-", td$month, "-01"))
dx <- Dates[!(Dates$date %in% td$date),]
dx
```



```{r}
dx$mean_evi <- NA
tdx <- rbind(td, dx) %>% 
  arrange(date)
head(tdx)
write.csv(tdx,"Data/NDVI.csv")
na.cnt <- length(tdx[is.na(tdx)])
evi.trend$na.cnt.2[i] <- na.cnt #add count of na values to dataframe
rm(td, dx) #remove data we're no longer using, this is a good rule of thumb, especially when working with larger datasets.
tdx <- ts(data = tdx$mean_evi, start = c(2000, 1), end = c(2022, 01), frequency = 12) #convert data to time series.
plot(tdx,pch = 16, 
     xlab = "Time", ylab = "EVI ", col = "#2E9FDF")
```


```{r}
tdx <- if(na.cnt > 0){imputeTS::na_kalman(tdx, model = "auto.arima", smooth = T)} else {
    tdx
}
plot(tdx,pch = 16, frame = FALSE,
     xlab = "Time", ylab = "EVI ", col = "#2E9FDF")

```


```{r}
tdx.dcp <- stl(tdx, s.window = 'periodic')
plot(tdx.dcp,pch = 16, frame = FALSE, col = "#2E9FDF")
```
```{r}
Tt <- trendcycle(tdx.dcp)
St <- seasonal(tdx.dcp)
Rt <- remainder(tdx.dcp)
plot(Rt)
plot(Tt)
plot(St)
```

```{r}
#| label: fig-ACF
#| fig-cap: "ACF Plot and PACF plot analysis for sample between 2000 and 2020:"
#| fig-subcap:
#|   - "Stationary Signal"
#|   - "Trend Signal"
#| layout-ncol: 2
#| column: page-right
# The Stationary Signal and ACF
plot(Rt,col= "red", main = "Stationary Signal")
acf(Rt, lag.max = length(Rt),
    xlab = "lag", ylab = 'ACF', main = '')

#The Trend Signal anf ACF

plot(Tt,col= "red",main = "Trend Signal")
acf(Tt, lag.max = length(Tt),
    xlab = "lag", ylab = "ACF", main = '')
```

```{r,warning=FALSE,message=FALSE}
tseries::adf.test(tdx)
```
```{r}
tdx.ns <- data.frame(time = c(1:length(tdx)), trend = tdx - tdx.dcp$time.series[,1])
Tt <- trendcycle(tdx.dcp)
St <- seasonal(tdx.dcp)
Rt <- remainder(tdx.dcp)
trend.summ <- summary(lm(formula = trend ~ time, data = tdx.ns)) #tslm
plot(tdx.ns,pch = 16, 
     xlab = "Time", ylab = "Trend ", col = "#2E9FDF")
abline(a = trend.summ$coefficients[1,1], b = trend.summ$coefficients[2,1], col='red')
```
```{r}
evi.trend$trend[i] <- trend.summ$coefficients[2,1]
evi.trend$trnd.strngth[i] <- round(max(0,1 - (var(Rt)/var(Tt + Rt))), 1) #Trend Strength Calculation <https://towardsdatascience.com/rainfall-time-series-analysis-and-forecasting-87a29316494e>
evi.trend$seas.strngth[i] <- round(max(0,1 - (var(Rt)/var(St + Rt))), 1) #Seasonal Strength Calculation
evi.trend$p.val[i] <- trend.summ$coefficients[2,4]
evi.trend$r2[i] <- trend.summ$r.squared
evi.trend$std.er[i] <- trend.summ$sigma
evi.trend[i,]
```


```{r}
plot(evi.hw <- forecast::hw(y = tdx, h = 12, damped = T))
```


```{r, eval=FALSE}

evi.trend <- read.csv("Data/rgeedf.csv")

for(i in 1:nrow(evi.df)){
tsv <- data.frame(evi = t(evi.df[i, 2:ncol(evi.df)])) 
colnames(tsv) <- c("evi")
na.cnt <- length(tsv[is.na(tsv)])
evi.trend$na.cnt[i] <- na.cnt
if(na.cnt < 263){
td <- tsv %>% 
  mutate(month = month(as.Date(rownames(tsv))), year = year(as.Date(rownames(tsv)))) %>%
  group_by(year, month) %>%
  summarise(mean_evi = mean(evi, na.rm = T), .groups = "keep") %>%
  as.data.frame()
td$date <- as.Date(paste0(td$year, "-", td$month, "-01"))
dx <- Dates[!(Dates$date %in% td$date),]
dx$mean_evi <- NA
tdx <- rbind(td, dx) %>% 
  arrange(date)
na.cnt <- length(tdx[is.na(tdx)])
evi.trend$na.cnt.2[i] <- na.cnt
rm(td, dx)
tdx <- ts(data = tdx$mean_evi, start = c(2001, 1), end = c(2019, 11), frequency = 12)
tdx <- if(na.cnt > 0){imputeTS::na_kalman(tdx, model = "auto.arima", smooth = T)} else {
    tdx
}

tdx.dcp <- stl(tdx, s.window = 'periodic')
evi.dcmp.lst[[i]] <- tdx.dcp
 #This will save our decomposition plots
plot(tdx.dcp)
dev.off()
tdx.ns <- data.frame(time = c(1:length(tdx)), trend = tdx - tdx.dcp$time.series[,1])
Tt <- trendcycle(tdx.dcp)
St <- seasonal(tdx.dcp)
Rt <- remainder(tdx.dcp)
trend.summ <- summary(lm(formula = trend ~ time, data = tdx.ns)) #tslm
evi.trend$trend[i] <- trend.summ$coefficients[2,1]
evi.trend$trnd.strngth[i] <- round(max(0,1 - (var(Rt)/var(Tt + Rt))), 1) 

evi.trend$seas.strngth[i] <- round(max(0,1 - (var(Rt)/var(St + Rt))), 1) #Seasonal Strength Calculation
evi.trend$p.val[i] <- trend.summ$coefficients[2,4]
evi.trend$r2[i] <- trend.summ$r.squared
evi.trend$std.er[i] <- trend.summ$sigma
evi.hw <- forecast::hw(y = tdx, h = 12, damped = T)
evi.hw.lst[[i]] <- evi.hw
# plot(evi.hw)
# dev.off()
# rm(evi.hw, trend.summ, tdx.ns, tdx.dcp, Tt, St, Rt, tdx, na.cnt)
# gc()
} else {
  evi.ts[[i]] <- NA
    }
  }


head(evi.trend) #Let's take a peak
```

And plot a density plot showing the spread of trend values in county

```{r}
# library(ggdensity)
# ggdensity(evi.trend, x = "trend", 
#           fill = "#0073C2FF", 
#           color = "#0073C2FF",
#           add = "mean", 
#           rug = TRUE)
write.csv(evi.trend,"Data/evi.trend.csv")
```
```{r}
evi.trend$system.index <- cc.evi[,1]
hex_trend <- hex %>%
  left_join(evi.trend, by = 'hex_id', keep = F) %>%
  replace(is.na(.), 0)
hex_trend <- st_transform(hex_trend, st_crs(4326))
```

**create a Leaflet Web Map!**

```{r}
library(classInt)
trend_brks <- classIntervals(hex_trend$trend, n=11, style = "fisher")
colorscheme <- RColorBrewer::brewer.pal(n = 11, 'RdYlGn')
palette_sds <- leaflet::colorBin(colorscheme, domain = hex_trend$trend, bins=trend_brks$brks, na.color = "#ffffff", pretty = T)

pop <- paste0("<b> Hex ID: </b>",hex_trend$hex_id,"<br><b>NA Count: </b>",hex_trend$na.cnt+hex_trend$na.cnt.2,"<br><b>Trend: </b>",format(round(hex_trend$trend, 4), scientific = FALSE),"<br><b> P-Value: </b>",round(hex_trend$p.val, 4),"<br><b>R2: </b>",round(hex_trend$r2, 4),"<br><b>Std Err: </b>",round(hex_trend$std.er, 4),"<br><b>Trend Strength: </b>",round(hex_trend$trnd.strngth, 2),"<br><b>Seasonal Strength: </b>",round(hex_trend$seas.strngth, 4),"<br>")
#Here we're creating a popup for our interactive map.
```

```{r}
library(leaflet)
library(dplyr)
map <- hex_trend %>%
  leaflet() %>%
  setView(5.96475,-1.782181, 9) %>%
  addProviderTiles("Esri.WorldTopoMap", group = "Topo Map") %>%
  addProviderTiles("Esri.WorldImagery", group = "Imagery", 
                   options = providerTileOptions(opacity = 0.7)) %>%
  addPolygons(
    fillColor = ~palette_sds(hex_trend$trend),
    fillOpacity = hex_trend$trnd.strngth,
    opacity = 0.5,
    weight = 0.1,
    color='white', 
    group = "Hexbins", 
    highlightOptions = highlightOptions(
       color = "white",
       weight = 2,
       bringToFront = TRUE),
       popup = pop,
    popupOptions = popupOptions(
       maxHeight = 250, 
       maxWidth = 250)) %>%
  addLegend(
    title = "Trend: lm(EVI ~ Month)",
    pal = palette_sds,
    values = hex_trend$trend,
    opacity = 0.7,
    labFormat = labelFormat(
      digits = 5)) %>%
  addLayersControl(
       baseGroups = c("Topo Map", "Imagery"),
       overlayGroups = c("Hexbins"),
       options = layersControlOptions(collapsed = FALSE)) %>%
  addScaleBar(position='bottomleft')
    
map
```

#############################################################################################################
# Import Data
```{r}
DATA <- read.csv("Data/Data.csv")%>%
  dplyr::select(date,mean_evi,Precipitation,Evapotranspiration,
                MiniTemperature,MaxTemperature,Humidity,Drought)
head(DATA)
```
# Check Missing Values
```{r}
visdat::vis_dat(DATA)
skimr::skim_tee(DATA)
summary(DATA)
```
# Correlation
```{r}
colnames(DATA)<- c("Date","EVI","Precipitation","Evapiration","TempMin","TempMax","Humidity","Drought")
plot(corr_coef(DATA))
```
# Select a set of predictors with minimal multicollinearity
```{r}
non_collinear_vars(DATA,TempMin,TempMax,Evapiration,Precipitation,Humidity,Drought,max_vif =3)
```
# Removed correlated variable
```{r}
DATA <- DATA%>%
dplyr::select(Date,Drought, TempMin, Precipitation, TempMax, Evapiration,EVI)

colnames(DATA)
head(DATA)
```
# Linear Model
```{r}
# summary(lm(mean_evi~Precipitation+Evapiration+MiniTemperature+MaxTemperature+Drought,data = DATA))

```
# Input Missing Values 
```{r}
DATA$EVI <- if(is.na(DATA$EVI) > 0){imputeTS::na_kalman(DATA$EVI, model = "auto.arima", smooth = T)} else {
    DATA$EVI
}
head(DATA)
```
```{r}
# ggplot2::ggplot(DATA,aes(Date,EVI))+
#   geom_plot()+
#   geom_abline()
```

# Check if Impute worked
```{r}
vis_dat(DATA)
```
```{r}


```
```{r}
df<- DATA%>%select(-Date)
# 1st differenced data
# df <- as.data.frame(diff(as.matrix(DATA), lag = 1))
#========================================================
# Summary of ADF test of level variables
#========================================================
 
adf.none  <- list(
    EVI = ur.df(DATA$EVI, type='none', selectlags = c("BIC")),
    Precipitation = ur.df(DATA$Precipitation, type='none', selectlags = c("BIC")),
    Evapiration = ur.df(DATA$Evapiration, type='none', selectlags = c("BIC")),
    TempMin = ur.df(DATA$TempMin, type='none', selectlags = c("BIC")),
    TempMax = ur.df(DATA$TempMax, type='none', selectlags = c("BIC")),
    Drought = ur.df(DATA$Drought, type='none', selectlags = c("BIC"))
    )
adf.drift  <- list(
    EVI = ur.df(DATA$EVI, type='drift', selectlags = c("BIC")),
    Precipitation = ur.df(DATA$Precipitation, type='drift', selectlags = c("BIC")),
    Evapiration = ur.df(DATA$Evapiration, type='drift', selectlags = c("BIC")),
    TempMin = ur.df(DATA$TempMin, type='drift', selectlags = c("BIC")),
    TempMax = ur.df(DATA$TempMax, type='drift', selectlags = c("BIC")),
    Drought = ur.df(DATA$Drought, type='drift', selectlags = c("BIC"))
    )
adf.trend  <- list(
    EVI = ur.df(DATA$EVI, type='trend', selectlags = c("BIC")),
    Precipitation = ur.df(DATA$Precipitation, type='trend', selectlags = c("BIC")),
    Evapiration = ur.df(DATA$Evapiration, type='trend', selectlags = c("BIC")),
    TempMin = ur.df(DATA$TempMin, type='trend', selectlags = c("BIC")),
    TempMax = ur.df(DATA$TempMax, type='trend', selectlags = c("BIC")),
    Drought = ur.df(DATA$Drought, type='trend', selectlags = c("BIC"))
    )

```

```{r}
# 1st differenced data
df <- as.data.frame(diff(as.matrix(DATA[,-1]), lag = 1))
df.adf.none  <- list(
    EVI = ur.df(df$EVI, type='none', selectlags = c("BIC")),
    Precipitation = ur.df(df$Precipitation, type='none', selectlags = c("BIC")),
    Evapiration = ur.df(df$Evapiration, type='none', selectlags = c("BIC")),
    TempMin = ur.df(df$TempMin, type='none', selectlags = c("BIC")),
    TempMax = ur.df(df$TempMax, type='none', selectlags = c("BIC")),
    Drought = ur.df(df$Drought, type='none', selectlags = c("BIC"))
    )
df.adf.drift  <- list(
    EVI = ur.df(df$EVI, type='drift', selectlags = c("BIC")),
    Precipitation = ur.df(df$Precipitation, type='drift', selectlags = c("BIC")),
    Evapiration = ur.df(df$Evapiration, type='drift', selectlags = c("BIC")),
    TempMin = ur.df(df$TempMin, type='drift', selectlags = c("BIC")),
    TempMax = ur.df(df$TempMax, type='drift', selectlags = c("BIC")),
    Drought = ur.df(df$Drought, type='drift', selectlags = c("BIC"))
    )
df.adf.trend  <- list(
    EVI = ur.df(df$EVI, type='trend', selectlags = c("BIC")),
    Precipitation = ur.df(df$Precipitation, type='trend', selectlags = c("BIC")),
    Evapiration = ur.df(df$Evapiration, type='trend', selectlags = c("BIC")),
    TempMin = ur.df(df$TempMin, type='trend', selectlags = c("BIC")),
    TempMax = ur.df(df$TempMax, type='trend', selectlags = c("BIC")),
    Drought = ur.df(df$Drought, type='trend', selectlags = c("BIC"))
    )
```
The ADF result for LRM variable from the above R code is generated as follows

```{r}
summary(adf.trend)
summary(df.adf.trend)
```
```{r}
adf.trend$EVI
```
Interpretation of ADF test follow the general-to-specific approach. As such, three regression models are applied sequentially.
```{r}
#========================================================
# General-to-Specific Investigation
# The case of EVI variable
#========================================================
 
print("Level Variable with Trend")
cbind(t(adf.trend$EVI@teststat), adf.trend$EVI@cval)

print("Level Variable with Trend")
cbind(t(adf.trend$Precipitation@teststat), adf.trend$Precipitation@cval)

print("Level Variable with Trend")
cbind(t(adf.trend$Evapiration@teststat), adf.trend$Evapiration@cval)

print("Level Variable with Trend")
cbind(t(adf.trend$TempMin@teststat), adf.trend$TempMin@cval)

print("Level Variable with Trend")
cbind(t(adf.trend$TempMax@teststat), adf.trend$TempMax@cval)

print("Level Variable with Trend")
cbind(t(adf.trend$Drought@teststat), adf.trend$Drought@cval)
```


```{r}
print("1st Diff. Variable with Drift and Trend")
cbind(t(df.adf.trend$EVI@teststat), df.adf.trend$EVI@cval)

print("1st Diff. Variable with Drift")
cbind(t(df.adf.drift$EVI@teststat), df.adf.drift$EVI@cval)

print("1st Diff. Variable with None")
cbind(t(df.adf.none$EVI@teststat), df.adf.none$EVI@cval)
```
We can get the following summarized ADF test results for level and 1st difference variable of

phi2 is insignificant : unit root(O), drift(X), trend(X)
phi3 is insignificant : unit root(O), drift(X)
tau3 is insignificant : unit root(O)

From the phi2(ϕ2ϕ2)-statistic, joint null hypothesis is not rejected so that there is a unit root and we can exclude drift and trend terms. The phi3(ϕ3ϕ3)-statistic shows that there is a unit root and we can exclude a drift term. Finally, the tau3(τ3τ3)-statistic shows that there is a unit root.

The following test statistics are consistent with the above results and we can use a ADF test without a drift and trend terms.

phi1 is insignificant : unit root(O), drift(X)
tau2 is insignificant : unit root(O)

tau1 is insignificant : unit root(O)


For the case of the LRY 1st diff. variable (blue box), the same reasoning is used and we can find that there is no unit root but drift and trend terms are necessary.

phi2 is significant : unit root(X), drift(O), trend(O)
phi3 is significant : unit root(X), drift(O)
tau3 is significant : unit root(X)

phi1 is significant : unit root(X), drift(O)
tau2 is significant : unit root(X)

tau1 is significant : unit root(X)


Finally, we can conclude that logarithm of real income contains a unit root and can be stationay time series by differencing the first order. Now that this transformed variable contains no unit root, it can be included in VAR or VECM model.

In this process, the alphanumeric names of test statistics are a little confusing but when we refer the above three specifications of regression equations, the meanings of names of test statistics are clear.
# Time Series using  VAR
```{r}
EVI <- ts(data = DATA$EVI, start = c(2000, 1), end = c(2022, 01), frequency = 12)
Precipitation <- ts(data = DATA$Precipitation, start = c(2000, 1), end = c(2022, 01), frequency = 12)
Evapiration <- ts(data = DATA$Evapiration, start = c(2000, 1), end = c(2022, 01), frequency = 12)
TempMin <- ts(data = DATA$TempMin, start = c(2000, 1), end = c(2022, 01), frequency = 12)
TempMax <- ts(data = DATA$TempMax, start = c(2000, 1), end = c(2022, 01), frequency = 12)
Drought <- ts(data = DATA$Drought, start = c(2000, 1), end = c(2022, 01), frequency = 12)
TimeSeries <- cbind(EVI,Precipitation,Evapiration,TempMin,TempMax,Drought)
```

```{r}

{
Matrix <- diag(6)
Matrix[2,1]<- NA
Matrix[3,1]<- NA
Matrix[4,1]<- NA
Matrix[5,1]<- NA
Matrix[6,1]<- NA
Matrix[3,2]<- NA
Matrix[4,2]<- NA
Matrix[5,2]<- NA
Matrix[6,2]<- NA
Matrix[4,3]<- NA
Matrix[5,3]<- NA
Matrix[6,]<- NA
Matrix[5,4]<- NA
Matrix[6,4]<- NA
Matrix[6,5]<- NA
}
```

```{r}

```

```{r}
library(TSstudio)
ts_plot(EVI)
ts_plot(Evapiration)
ts_plot(Drought)
plot(TimeSeries)

```
# Lag Selection 
```{r}
LagSelection <-VARselect(TimeSeries, 
          type = "const", #type of deterministic regressors to include. We use none becasue the time series was made stationary using differencing above. 
          lag.max = 12) #highest lag order
LagSelection$selection
kable(t(LagSelection$criteria))%>%kable_styling(latex_options = c("repeat_header"))                #information criterion
```


# Estemating Models
```{r}
# Creating a VAR model with vars
Model1 <- VAR(TimeSeries,p= 8,lag.max = 12,season = NULL,  exogen = NULL,type = "const")
```

```{r}
predict(Model1, n.ahead = 12, ci = 0.95)
```
forecast is a generic function for forecasting from time series or time series models. The function invokes particular methods which depend on the class of the first argument.
```{r}
forecast(Model1)
plot(forecast(Model1))
```
```{r}
accuracy(forecast(Model1),d=1, D= 1)
```

```{r}
library(vars)
colnames(TimeSeries) <-c("EVI","Prep","Evap","TMin","TMax","Drght")
Model2 <- VAR(TimeSeries,p= 8,lag.max = 12,season = NULL,  exogen = NULL,type = "const")
```

```{r}
plot.varfevd  <-function (x, plot.type = c("multiple", "single"), names = NULL,
    main = NULL, col = NULL, ylim = NULL, ylab = NULL, xlab = NULL,
    legend = NULL, names.arg = NULL, nc, mar = par("mar"), oma = par("oma"),
    addbars = 1, ...)
{
    K <- length(x)
    ynames <- names(x)
    plot.type <- match.arg(plot.type)
    if (is.null(names)) {
        names <- ynames
    }
    else {
        names <- as.character(names)
        if (!(all(names %in% ynames))) {
            warning("\nInvalid variable name(s) supplied, using first variable.\n")
            names <- ynames[1]
        }
    }
    nv <- length(names)
#    op <- par(no.readonly = TRUE)
    ifelse(is.null(main), main <- paste("FEVD for", names), main <- rep(main,
        nv)[1:nv])
    ifelse(is.null(col), col <- gray.colors(K), col <- rep(col,
        K)[1:K])
    ifelse(is.null(ylab), ylab <- rep("Percentage", nv), ylab <- rep(ylab,
        nv)[1:nv])
    ifelse(is.null(xlab), xlab <- rep("Horizon", nv), xlab <- rep(xlab,
        nv)[1:nv])
    ifelse(is.null(ylim), ylim <- c(0, 1), ylim <- ylim)
    ifelse(is.null(legend), legend <- ynames, legend <- legend)
    if (is.null(names.arg))
        names.arg <- c(paste(1:nrow(x[[1]])), rep(NA, addbars))
    plotfevd <- function(x, main, col, ylab, xlab, names.arg,
        ylim, ...) {
        addbars <- as.integer(addbars)
        if (addbars > 0) {
            hmat <- matrix(0, nrow = K, ncol = addbars)
            xvalue <- cbind(t(x), hmat)
            barplot(xvalue, main = main, col = col, ylab = ylab,
                xlab = xlab, names.arg = names.arg, ylim = ylim,
                legend.text = legend, ...)
            abline(h = 0)
        }
        else {
            xvalue <- t(x)
            barplot(xvalue, main = main, col = col, ylab = ylab,
                xlab = xlab, names.arg = names.arg, ylim = ylim,
                ...)
            abline(h = 0)
        }
    }
    if (plot.type == "single") {
#        par(mar = mar, oma = oma)
#        if (nv > 1)
#            par(ask = TRUE)
        for (i in 1:nv) {
            plotfevd(x = x[[names[i]]], main = main[i], col = col,
                ylab = ylab[i], xlab = xlab[i], names.arg = names.arg,
                ylim = ylim, ...)
        }
    }
    else if (plot.type == "multiple") {
        if (missing(nc)) {
            nc <- ifelse(nv > 4, 2, 1)
        }
        nr <- ceiling(nv/nc)
        par(mfcol = c(nr, nc), mar = mar, oma = oma)
        for (i in 1:nv) {
            plotfevd(x = x[[names[i]]], main = main[i], col = col,
                ylab = ylab[i], xlab = xlab[i], names.arg = names.arg,
                ylim = ylim, ...)
        }
    }
#    on.exit(par(op))
}
```

```{r}
win.graph(width=13,height=8)
layout(matrix(1:6,ncol=1))
plot.varfevd(fevd(Model2, n.ahead = 10 ),plot.type = "multiple", col=1:6)
```

```{r}
fevd <- fevd(Model1)
plot(fevd$EVI)

```



# Impuse Repones Analysis
```{r}
plot(irf(Model1,impulse = "EVI",response = "EVI"))
plot(irf(Model1,impulse = "EVI",response = "Precipitation"))
plot(irf(Model1,impulse = "EVI",response = "Evapiration"))
plot(irf(Model1,impulse = "EVI",response = "TempMin"))
plot(irf(Model1,impulse = "EVI",response = "TempMax"))
plot(irf(Model1,impulse = "EVI",response = "Drought"))
```
```{r}
grangertest(EVI~Precipitation, order = 12)
grangertest(EVI~TempMin, order = 12)
grangertest(EVI~TempMax, order = 12)
grangertest(EVI~Evapiration, order = 12)
grangertest(EVI~Drought, order = 12)
```


```{r}
data <- DATA%>%select(Date,Drought, TempMin, Precipitation, TempMax, Evapiration,EVI)
# Make a tsibble for modeling
to_model <- data %>% 
  mutate(dates = yearmonth(paste0(Date, "-01"))) %>% 
  as_tibble() %>% 
  as_tsibble()
```

```{r}
# train_test_split <- 0.65
# 
# training <- to_model %>% 
#   slice(1:I(nrow(to_model) 
#             ))
# 
# test <- to_model %>% 
#   slice(I(nrow(to_model) * train_test_split + 1):I(nrow(to_model) + 1))


```

```{r}
# Split into training and test sets ---------------------------------------

library(rsample)
Split <- initial_time_split(to_model,prop = 0.7,lag=12)
train <- training(Split)
test <- testing(Split)
# Get the split date for the bias-free test set
split_date <- min(test$dates) + 120
```


```{r}
# Train models ------------------------------------------------------------

# Different formulas for arima and rest of the models


f_mc <- as.formula("EVI~Drought+ TempMin+ Precipitation+ TempMax+ Evapiration")

# Train six models plus make a combination
models <- train %>% 
  model(ARIMA = ARIMA(f_mc, stepwise = FALSE),
        nnetar = NNETAR(f_mc),
        VAR = VAR(f_mc),
        Prophet = prophet(f_mc),
        ETS = ETS(EVI),
        TSLM = TSLM(EVI~Drought+ TempMin+ Precipitation+ TempMax+ Evapiration+
                      trend() + season())) %>% 
  mutate(Combination = (ARIMA + nnetar + VAR + Prophet + TSLM) / 5)
```

```{r}
models
```

```{r}
# Plot the models ---------------------------------------------------------

# Make forecasts
fcasts <- models %>% forecast(test)

# Calculate accuracies for the test set
# rbind training set to get MASE calculations
accuracies <- fcasts %>% accuracy(rbind(training,
                                        test %>% filter(dates >= split_date)))

# Calculate R-squareds, NA if near-zero variance
accuracies <- accuracies %>% 
  inner_join(fcasts %>% 
               as_tibble() %>% 
               group_by(.model) %>% 
               summarise(rsq = ifelse(sd(tenyear) > 0.01,
                                      cor(tenyear, test$tenyear)^2,
                                      NA)))
```





